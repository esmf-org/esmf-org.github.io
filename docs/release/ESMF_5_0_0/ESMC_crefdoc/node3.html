<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">

<!--Converted with LaTeX2HTML 2002 (1.67)
original version by:  Nikos Drakos, CBLU, University of Leeds
* revised and updated by:  Marcus Hennecke, Ross Moore, Herb Swan
* with significant contributions from:
  Jens Lippmann, Marek Rouchal, Martin Wilck and others -->
<HTML>
<HEAD>
<TITLE>2 Superstructure</TITLE>
<META NAME="description" CONTENT="2 Superstructure">
<META NAME="keywords" CONTENT="ESMC_crefdoc">
<META NAME="resource-type" CONTENT="document">
<META NAME="distribution" CONTENT="global">

<META HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=iso-8859-1">
<META NAME="Generator" CONTENT="LaTeX2HTML v2002">
<META HTTP-EQUIV="Content-Style-Type" CONTENT="text/css">

<LINK REL="STYLESHEET" HREF="ESMC_crefdoc.css">

<LINK REL="next" HREF="node4.html">
<LINK REL="previous" HREF="node2.html">
<LINK REL="up" HREF="ESMC_crefdoc.html">
<LINK REL="next" HREF="node4.html">
</HEAD>

<BODY BGCOLOR=white LINK=#083194 VLINK=#21004A>
<!--Navigation Panel-->
<A NAME="tex2html267"
  HREF="node4.html">
<IMG WIDTH="37" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="next" SRC="next.png"></A> 
<A NAME="tex2html263"
  HREF="ESMC_crefdoc.html">
<IMG WIDTH="26" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="up" SRC="up.png"></A> 
<A NAME="tex2html257"
  HREF="node2.html">
<IMG WIDTH="63" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="previous" SRC="prev.png"></A> 
<A NAME="tex2html265"
  HREF="node1.html">
<IMG WIDTH="65" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="contents" SRC="contents.png"></A>  
<BR>
<B> Next:</B> <A NAME="tex2html268"
  HREF="node4.html">3 Infrastructure: Fields and</A>
<B> Up:</B> <A NAME="tex2html264"
  HREF="ESMC_crefdoc.html">ESMC_crefdoc</A>
<B> Previous:</B> <A NAME="tex2html258"
  HREF="node2.html">1 ESMF Overview</A>
 &nbsp <B>  <A NAME="tex2html266"
  HREF="node1.html">Contents</A></B> 
<BR>
<BR>
<!--End of Navigation Panel-->
<!--Table of Child-Links-->
<A NAME="CHILD_LINKS"><STRONG>Subsections</STRONG></A>

<UL>
<LI><UL>
<LI><A NAME="tex2html269"
  HREF="node3.html#SECTION03010000000000000000">3 Overview of Superstructure</A>
<UL>
<LI><A NAME="tex2html270"
  HREF="node3.html#SECTION03011000000000000000">3.1 Superstructure Classes</A>
<LI><A NAME="tex2html271"
  HREF="node3.html#SECTION03012000000000000000">3.2 Hierarchical Creation of Components</A>
<LI><A NAME="tex2html272"
  HREF="node3.html#SECTION03013000000000000000">3.3 Sequential and Concurrent Execution of Components</A>
<LI><A NAME="tex2html273"
  HREF="node3.html#SECTION03014000000000000000">3.4 Intra-Component Communication</A>
<LI><A NAME="tex2html274"
  HREF="node3.html#SECTION03015000000000000000">3.5 Data Distribution and Scoping in Components</A>
<LI><A NAME="tex2html275"
  HREF="node3.html#SECTION03016000000000000000">3.6 Performance</A>
<LI><A NAME="tex2html276"
  HREF="node3.html#SECTION03017000000000000000">3.7 Object Model</A>
</UL>
<LI><A NAME="tex2html277"
  HREF="node3.html#SECTION03020000000000000000">4 Application Driver and Required ESMF Methods</A>
<UL>
<LI><A NAME="tex2html278"
  HREF="node3.html#SECTION03021000000000000000">4.1 Description</A>
<LI><A NAME="tex2html279"
  HREF="node3.html#SECTION03022000000000000000">4.2 Required ESMF Methods</A>
<UL>
<LI><A NAME="tex2html280"
  HREF="node3.html#SECTION03022100000000000000">4.2.1 ESMC_Initialize</A>
</UL>
</UL>
<LI><A NAME="tex2html281"
  HREF="node3.html#SECTION03030000000000000000">5 GridComp Class</A>
<UL>
<LI><A NAME="tex2html282"
  HREF="node3.html#SECTION03031000000000000000">5.1 Description</A>
<LI><A NAME="tex2html283"
  HREF="node3.html#SECTION03032000000000000000">5.2 Class API</A>
<LI><A NAME="tex2html284"
  HREF="node3.html#SECTION03033000000000000000">5.3 C++:  Class Interface ESMC_Comp - Public C interface to the ESMF Comp class (Source File: ESMC_Comp.h)</A>
</UL>
<LI><A NAME="tex2html285"
  HREF="node3.html#SECTION03040000000000000000">6 CplComp Class</A>
<UL>
<LI><A NAME="tex2html286"
  HREF="node3.html#SECTION03041000000000000000">6.1 Description</A>
</UL>
<LI><A NAME="tex2html287"
  HREF="node3.html#SECTION03050000000000000000">7 State Class</A>
<UL>
<LI><A NAME="tex2html288"
  HREF="node3.html#SECTION03051000000000000000">7.1 Description</A>
<LI><A NAME="tex2html289"
  HREF="node3.html#SECTION03052000000000000000">7.2 Class API</A>
<LI><A NAME="tex2html290"
  HREF="node3.html#SECTION03053000000000000000">7.3 C++:  Class Interface ESMC_State - C interface to the F90 State object (Source File: ESMC_State.h)</A>
</UL></UL></UL>
<!--End of Table of Child-Links-->
<HR>

<H1><A NAME="SECTION03000000000000000000">
2 Superstructure</A>
</H1>

<P>
<A NAME="part:Superstructure"></A>
<P>

<P>

<P>

<H1><A NAME="SECTION03010000000000000000">
3 Overview of Superstructure</A>
</H1>

<P>
ESMF superstructure classes define an architecture for assembling
Earth system applications from modeling <B>components</B>.  A component
may be defined in terms of the physical domain that it represents,
such as an atmosphere or sea ice model.  It may also be defined in terms
of a computational function, such as a data assimilation system.
Earth system research often requires that such components be <B>coupled</B> 
together to create an application.  By coupling we mean the data 
transformations and, on parallel computing systems, data transfers, 
that are necessary to allow data from one component to be utilized by 
another.  ESMF offers regridding methods and other tools to simplify 
the organization and execution of inter-component data exchanges.  

<P>
In addition to components defined at the level of major physical 
domains and computational functions, components may be defined that 
represent smaller computational functions within larger components, 
such as the transformation of data between the physics and dynamics 
in a spectral atmosphere model, 
or the creation of nested higher resolution regions 
within a coarser grid.  The objective is to couple components at varying 
scales both flexibly and efficiently.  ESMF encourages a hierachical
application structure, in which large components branch into 
smaller sub-components (see Figure <A HREF="node3.html#fig:GEOS5">2</A>).  ESMF also makes 
it easier for the same component to be used in multiple contexts 
without changes to its source code.

<P>
<DIV ALIGN="CENTER">
<TABLE CELLPADDING=3 BORDER="1">
<TR><TD ALIGN="LEFT" VALIGN="TOP" WIDTH=432>
<BR>
<B>Key Features</B></TD>
</TR>
<TR><TD ALIGN="LEFT" VALIGN="TOP" WIDTH=432>Modular, component-based architecture.</TD>
</TR>
<TR><TD ALIGN="LEFT" VALIGN="TOP" WIDTH=432>Hierarchical assembly of components into applications.</TD>
</TR>
<TR><TD ALIGN="LEFT" VALIGN="TOP" WIDTH=432>Use of components in multiple contexts without modification.</TD>
</TR>
<TR><TD ALIGN="LEFT" VALIGN="TOP" WIDTH=432>Sequential or concurrent component execution.</TD>
</TR>
<TR><TD ALIGN="LEFT" VALIGN="TOP" WIDTH=432>Single program, multiple datastream (SPMD) applications for 
maximum portability and reconfigurability.</TD>
</TR>
<TR><TD ALIGN="LEFT" VALIGN="TOP" WIDTH=432>Multiple program, multiple datastream (MPMD) option for 
flexibility.</TD>
</TR>
</TABLE>
</DIV>

<P>

<H2><A NAME="SECTION03011000000000000000">
3.1 Superstructure Classes</A>
</H2>

<P>
There are a small number of classes in the ESMF superstructure:

<P>

<UL>
<LI><B>Component</B>  An ESMF component has two parts, one that is 
supplied by the ESMF and one that is supplied by the user.  The
part that is supplied by the framework is an ESMF derived type that
is either a Gridded Component (<B>GridComp</B>) or a Coupler 
Component (<B>CplComp</B>).  A Gridded Component typically represents
a physical domain in which data is associated with one or more 
grids - for example, a sea ice model.  A Coupler Component 
arranges and executes data transformations and transfers between
one or more Gridded Components. Gridded Components and Coupler 
Components have standard methods, which include initialize, run,
and finalize.  These methods can be multi-phase.

<P>
The second part of an ESMF Component is user code, such as a
model or data assimilation system.  Users set entry points 
within their code so that it is callable by the framework.  
In practice, setting entry points means that within user code 
there are calls to ESMF methods that associate the name of a 
Fortran subroutine with a corresponding standard ESMF operation.  
For example, a user-written initialization routine called 
<TT>myOceanInit</TT> might be associated with the standard 
initialize routine of an ESMF Gridded Component named ``myOcean'' 
that represents an ocean model.

<P>
</LI>
<LI><B>State</B>  ESMF components exchange information with other 
components only through States.  A State is an ESMF derived
type that can contain Fields, FieldBundles, Arrays, ArrayBundles,
and other States.  A Component is associated with two States, an 
<B>Import State</B> and an <B>Export State</B>.  Its Import State 
holds the data that it receives from other Components.  
Its Export State contains data that it can make available to 
other Components. 

<P>
</LI>
<LI><B>Application Driver</B> The Application Driver (<B>AppDriver</B>) 
is a small, generic driver program that contains the ``main'' 
routine for an ESMF application.

<P>
</LI>
</UL>

<P>
An ESMF coupled application typically involves an AppDriver, a parent 
Gridded Component, two or more child Gridded Components that require 
an inter-component data exchange, and one or more Coupler 
Components. 

<P>
The parent Gridded Component is responsible for creating the child 
Gridded Components that are exchanging data, for creating the Coupler, 
for creating the necessary Import and Export States, and for 
setting up the desired sequencing.  The AppDriver ``main'' routine
calls the parent Gridded Component's initialize, run, and finalize 
methods in order to execute the application.  For each of these
standard methods, the parent Gridded Component in turn calls the 
corresponding methods in the child Gridded Components and the 
Coupler Component.  For example, consider a simple coupled 
ocean/atmosphere simulation.  When the initialize method of the 
parent Gridded Component is called by the AppDriver, it in turn 
calls the initialize methods of its child atmosphere and ocean 
Gridded Components, and the initialize method of an 
ocean-to-atmosphere Coupler Component.  Figure <A HREF="node3.html#fig:appunit">3</A>
shows this schematically.

<P>
<DIV ALIGN="CENTER">
</DIV>
<DIV ALIGN="CENTER"><A NAME="fig:GEOS5"></A><A NAME="296"></A>
<TABLE>
<CAPTION ALIGN="BOTTOM"><STRONG>Figure 2:</STRONG>
ESMF enables applications such as the atmospheric general
circulation model GEOS-5 to be structured hierarchically, and 
reconfigured and extended easily.  Each box in this diagram is an
ESMF Gridded Component.</CAPTION>
<TR><TD><!-- MATH
 $\scalebox{1.0}{\includegraphics{ESMF_GEOS5}}$
 -->
<IMG
 WIDTH="807" HEIGHT="488" ALIGN="BOTTOM" BORDER="0"
 SRC="img2.png"
 ALT="\scalebox{1.0}{\includegraphics{ESMF_GEOS5}}"></TD></TR>
</TABLE>
</DIV>
<DIV ALIGN="CENTER">
</DIV>

<P>

<H2><A NAME="SECTION03012000000000000000"></A>
<A NAME="sec:hierarchy"></A>
<BR>
3.2 Hierarchical Creation of Components
</H2>

<P>
Components are allocated computational resources in the form of
<B>Persistent Execution Threads</B>, or <B>PET</B>s.  A list of a Component's
PETs is contained in a structure called a <B>Virtual Machine</B>,
or <B>VM</B>.  The VM also contains information about the topology and
characteristics of the underlying computer.
Components are created hierarchically, with parent Components creating
child Components and allocating some or all of their PETs to each one.
By default ESMF creates a new VM for each child Component, which 
allows Components to tailor their VM resources to match their needs.
In some cases a child may want to share its parent's VM - ESMF
supports this too.

<P>
A Gridded Component may exist across all the PETs in an application. 
A Gridded Component may also reside on a subset of PETs in an
application.  These PETs may wholly coincide with, be wholly contained
within, or wholly contain another Component.

<P>
<DIV ALIGN="CENTER">
</DIV>
<DIV ALIGN="CENTER"><A NAME="fig:appunit"></A><A NAME="310"></A>
<TABLE>
<CAPTION ALIGN="BOTTOM"><STRONG>Figure 3:</STRONG>
A call to a standard ESMF initialize (run, finalize) method
by a parent component triggers calls to initialize (run, finalize)
all of its child components.</CAPTION>
<TR><TD><!-- MATH
 $\scalebox{1.0}{\includegraphics{ESMF_appunit}}$
 -->
<IMG
 WIDTH="749" HEIGHT="547" ALIGN="BOTTOM" BORDER="0"
 SRC="img3.png"
 ALT="\scalebox{1.0}{\includegraphics{ESMF_appunit}}"></TD></TR>
</TABLE>
</DIV>
<DIV ALIGN="CENTER">
</DIV>

<P>

<H2><A NAME="SECTION03013000000000000000"></A>
<A NAME="sec:concurrency"></A>
<BR>
3.3 Sequential and Concurrent Execution of Components
</H2>

<P>
When a set of Gridded Components and a Coupler runs in sequence
on the same set of PETs the application is executing in a <B>sequential</B>
mode. When Gridded Components are created and run on mutually exclusive
sets of PETs, and are coupled by a Coupler Component that extends over
the union of these sets, the mode of execution is <B>concurrent</B>.

<P>
Figure <A HREF="node3.html#fig:serial">4</A> illustrates a typical configuration for 
a simple coupled sequential
application, and Figure <A HREF="node3.html#fig:concurrent">5</A> shows a possible 
configuration for the same application running in a concurrent mode.

<P>
Parent Components can select if and when to wait for concurrently
executing child Components, synchronizing only when required.

<P>
It is possible for ESMF applications to contain some Component sets
that are executing sequentially and others that are executing concurrently.
We might have, for example, atmosphere and land Components created
on the same subset of PETs, ocean and sea ice Components created on
the remainder of PETs, and a Coupler created across all the PETs in
the application.

<P>
<DIV ALIGN="CENTER">
</DIV>
<DIV ALIGN="CENTER"><A NAME="fig:serial"></A><A NAME="324"></A>
<TABLE>
<CAPTION ALIGN="BOTTOM"><STRONG>Figure 4:</STRONG>
Schematic of the run method of a coupled application, with an
``Atmosphere'' and an ``Ocean'' Gridded Component running sequentially with 
an ``Atm-Ocean Coupler.''  The top-level ``Hurricane Model'' 
Gridded Component contains the sequencing information and time 
advancement loop.  The AppDriver, Coupler, and all Gridded Components 
are distributed over nine PETs.</CAPTION>
<TR><TD><!-- MATH
 $\scalebox{1.0}{\includegraphics{ESMF_serial}}$
 -->
<IMG
 WIDTH="621" HEIGHT="704" ALIGN="BOTTOM" BORDER="0"
 SRC="img4.png"
 ALT="\scalebox{1.0}{\includegraphics{ESMF_serial}}"></TD></TR>
</TABLE>
</DIV>
<DIV ALIGN="CENTER">
</DIV>

<P>
<DIV ALIGN="CENTER">
</DIV>
<DIV ALIGN="CENTER"><A NAME="fig:concurrent"></A><A NAME="332"></A>
<TABLE>
<CAPTION ALIGN="BOTTOM"><STRONG>Figure 5:</STRONG>
Schematic of the run method of a coupled application, with an
``Atmosphere'' and an ``Ocean'' Gridded Component running concurrently with 
an ``Atm-Ocean Coupler.''  The top-level ``Hurricane Model'' 
Gridded Component contains the sequencing information and time 
advancement loop.  The AppDriver, Coupler, and top-level ``Hurricane
Model'' Gridded Component are distributed over nine PETs.  The
``Atmosphere'' Gridded Component is distributed over three PETs and
the ``Ocean'' Gridded Component is distributed over six PETs.</CAPTION>
<TR><TD><!-- MATH
 $\scalebox{1.0}{\includegraphics{ESMF_concurrent}}$
 -->
<IMG
 WIDTH="621" HEIGHT="603" ALIGN="BOTTOM" BORDER="0"
 SRC="img5.png"
 ALT="\scalebox{1.0}{\includegraphics{ESMF_concurrent}}"></TD></TR>
</TABLE>
</DIV>
<DIV ALIGN="CENTER">
</DIV>

<P>

<H2><A NAME="SECTION03014000000000000000"></A>
<A NAME="sec:localcomm"></A>
<BR>
3.4 Intra-Component Communication
</H2>

<P>
All data transfers within an ESMF application occur <I>within</I> a
component.  For example, a Gridded Component may contain halo updates.
Another example is that a Coupler Component may redistribute
data between two Gridded Components.  As a result,
the architecture of ESMF does not depend on any particular data
communication mechanism, and new communication schemes can be
introduced without affecting the overall structure of the application.

<P>
Since all data communication happens within a component, a Coupler
Component must be created on the union of the PETs of all
the Gridded Components that it couples.  

<P>

<H2><A NAME="SECTION03015000000000000000"></A>
<A NAME="sec:scoping"></A>
<BR>
3.5 Data Distribution and Scoping in Components
</H2>

<P>
The scope of distributed objects is the VM of the currently 
executing Component.  For this reason, all
PETs in the current VM must make the same distributed object
creation calls.   When a Coupler Component running on a superset
of a Gridded Component's PETs needs to make communication calls
involving objects created by the Gridded Component,
an ESMF-supplied function called <TT>ESMF_StateReconcile()</TT> creates proxy
objects for those PETs that had no previous information about the
distributed objects.  Proxy objects contain no local data but
can be used in communication calls (such as regrid or redistribute)
to describe the remote source for data being moved to the current PET,
or to describe the remote destination for data being moved from the local PET.
Figure <A HREF="node3.html#fig:reconcile">6</A> is a simple schematic that shows the 
sequence of events in a reconcile call.

<P>
<DIV ALIGN="CENTER">
</DIV>
<DIV ALIGN="CENTER"><A NAME="fig:reconcile"></A><A NAME="368"></A>
<TABLE>
<CAPTION ALIGN="BOTTOM"><STRONG>Figure 6:</STRONG>
An <TT>ESMF_StateReconcile()</TT> call creates proxy 
objects for use in subsequent communication calls.  The reconcile 
call would normally be made during Coupler initialization.</CAPTION>
<TR><TD><!-- MATH
 $\scalebox{1.0}{\includegraphics{ESMF_reconcile}}$
 -->
<IMG
 WIDTH="633" HEIGHT="662" ALIGN="BOTTOM" BORDER="0"
 SRC="img6.png"
 ALT="\scalebox{1.0}{\includegraphics{ESMF_reconcile}}"></TD></TR>
</TABLE>
</DIV>
<DIV ALIGN="CENTER">
</DIV>

<P>

<H2><A NAME="SECTION03016000000000000000"></A>
<A NAME="sec:performance"></A>
<BR>
3.6 Performance
</H2>

<P>
The ESMF design enables the user to configure ESMF
applications so that data is transferred directly from one component 
to another, without requiring that it be copied or sent to a different data
buffer as an interim step.  This is likely to be the most efficient way 
of performing inter-component coupling.  However, if desired, an 
application can also be configured so that data from a source component 
is sent to a distinct set of Coupler Component PETs for processing 
before being sent to its destination.

<P>
The ability to overlap computation with communication is essential for
performance.  When running with ESMF the user can initiate data 
sends during Gridded Component execution, as soon as the data is ready.
Computations can then proceed simultaneously with the data transfer.

<P>

<H2><A NAME="SECTION03017000000000000000">
3.7 Object Model</A>
</H2>

<P>
The following is a simplified UML diagram showing the relationships among
ESMF superstructure classes.  See Appendix A, <I>A Brief Introduction 
to UML</I>, for a translation table that lists the symbols in the diagram 
and their meaning.

<P>
<DIV ALIGN="CENTER">
<IMG
 WIDTH="692" HEIGHT="224" ALIGN="BOTTOM" BORDER="0"
 SRC="img7.png"
 ALT="\includegraphics[]{Comp_obj}">   

</DIV>

<P>

<P>

<P>

<H1><A NAME="SECTION03020000000000000000">
4 Application Driver and Required ESMF Methods</A>
</H1>

<P>

<H2><A NAME="SECTION03021000000000000000">
4.1 Description</A>
</H2>

<P>
The ESMF Application Driver (<TT>ESMF_AppDriver</TT>), is a generic ESMF 
driver program that contains a ``main.''  Simpler applications may be
able to use an Application Driver without modification; for more
complex applications, an Application Driver can be used as an extendable 
template.

<P>
ESMF provides a number of different Application Drivers in the 
<TT>$ESMF_DIR/src/Superstructure/AppDriver</TT> directory.
An appropriate one can be chosen depending on how the application
is to be structured.  Options when deciding how to structure an 
application include choices about:

<P>
<DL>
<DT><STRONG>Sequential vs. Concurrent Execution</STRONG></DT>
<DD><P>
In a sequential execution model every Component executes
on all PETs, with each Component completing execution before
the next Component begins.  This has the appeal of 
simplicity of data consumption and production: when a Gridded 
Component starts all required data is available for use, and when 
a Gridded Component finishes all data produced is ready for consumption 
by the next Gridded Component.  This approach also has
the possibility of less data movement if the grid and
data decomposition is done such that each processor's memory contains
the data needed by the next Component.

<P>
In a concurrent execution model subgroups of PETs run
Gridded Components and multiple Gridded Components are active at the 
same time.  Data exchange must be coordinated between Gridded 
Components so that data deadlock does not occur.  This strategy 
has the advantage of allowing coupling to other Gridded Components 
at any time during the computational process, including not 
having to return to the calling level of code before making 
data available.  

<P>
</DD>
<DT><STRONG>Pairwise vs. Hub and Spoke</STRONG></DT>
<DD><P>
Coupler Components are responsible for taking data from one
Gridded Component and putting it into the form expected by another 
Gridded Component.  This might include regridding, change of units, 
averaging, or binning.

<P>
Coupler Components can be written for <I>pairwise</I> data exchange: 
the Coupler Component takes data from a single Component and transforms 
it for use by another single Gridded Component.  This simplifies the 
structure of the Coupler Component code.

<P>
Couplers can also be written using a <I>hub and spoke</I> model where a
single Coupler accepts data from all other Components, can do data
merging or splitting, and formats data for all other Components.

<P>
Multiple Couplers, using either of the above two models or some mixture of
these approaches, are also possible.

<P>
</DD>
<DT><STRONG>Implementation Language</STRONG></DT>
<DD><P>
The ESMF framework currently has Fortran interfaces for all public functions. 
Some functions also have C interfaces, and the number of these is expected to 
increase over time. 

<P>
</DD>
<DT><STRONG>Number of Executables</STRONG></DT>
<DD><P>
The simplest way to run an application
is to run the same executable program on all PETs.  Different Components
can still be run on mutually exclusive PETs by using branching
(e.g., if this is PET 1, 2, or 3, run Component A, if it is
PET 4, 5, or 6 run Component B).  This is a <B>SPMD</B> model, 
Single Program Multiple Data.  

<P>
The alternative is to start a different executable program on different
PETs.  This is a <B>MPMD</B> model, Multiple Program Multiple Data.
There are complications with many job control systems on multiprocessor
machines in getting the different executables started, and getting
inter-process communcations established.  ESMF currently has some
support for MPMD: different Components can run as separate executables,
but the Coupler that transfers data between the Components must still
run on the union of their PETs. This means that the Coupler Component
must be linked into all of the executables.

<P>
</DD>
</DL>

<P>

<P>

<H2><A NAME="SECTION03022000000000000000">
4.2 Required ESMF Methods</A>
</H2>

<P>
There are a few methods that every ESMF application must contain. First,
<TT>ESMF_Initialize()</TT> and <TT>ESMF_Finalize()</TT> are in complete analogy 
to <TT>MPI_Init()</TT> and <TT>MPI_Finalize()</TT> known from MPI. All ESMF
programs, serial or parallel, must initialize the ESMF system at the beginning,
and finalize it at the end of execution. The behavior of calling any
ESMF method before <TT>ESMF_Initialize()</TT>, or after <TT>ESMF_Finalize()</TT>
is undefined.

<P>
Second, every ESMF Component that is accessed by an ESMF application requires
that its set services routine is called through
<TT>ESMF_&lt;Grid/Cpl&gt;CompSetServices()</TT>. The Component must implement
one public entry point, its set services routine, that can be called
through the <TT>ESMF_&lt;Grid/Cpl&gt;CompSetServices()</TT> library routine. The
Component set services routine is responsible for setting entry points
for the standard ESMF Component methods Initialize, Run, and Finalize.

<P>
Finally, the Component library call <TT>ESMF_&lt;Grid/Cpl&gt;CompSetVM()</TT> can
optionally be issues <EM>before</EM> calling
<TT>ESMF_&lt;Grid/Cpl&gt;CompSetServices()</TT>. Similar to 
<TT>ESMF_&lt;Grid/Cpl&gt;CompSetServices()</TT>, the <TT>ESMF_&lt;Grid/Cpl&gt;CompSetVM()</TT>
call requires a public entry point into the Component. It allows the Component
to adjust certain aspects of its execution environment, i.e. its own VM, before
it is started up.

<P>
The following sections discuss the above mentioned aspects in more detail.

<P>

<P>

<P>

<H3><A NAME="SECTION03022100000000000000">
4.2.1 ESMC_Initialize - Initialize the ESMF Framework</A>
</H3>

<P>

<P><P>
<BR>
<I>INTERFACE:</I>
<PRE>   int ESMC_Initialize(
</PRE><EM>RETURN VALUE:</EM>
<PRE>    int return code
</PRE><EM>ARGUMENTS:</EM>
<PRE>     int *rc,        // return code
     ...);           // optional arguments
 #define ESMC_InitArgDefaultConfigFilename(ARG)  \
 ESMCI_Arg(ESMCI_InitArgDefaultConfigFilenameID,ARG)
</PRE>
<I>DESCRIPTION:
<BR></I>

<P>
Initialize the ESMF.  This method must be called before
    any other ESMF methods are used.  The method contains a
    barrier before returning, ensuring that all processes
    made it successfully through initialization.

<P>
Typically <TT>ESMC_Initialize()</TT> will call <TT>MPI_Init()</TT> 
    internally unless MPI has been initialized by the user code before
    initializing the framework. If the MPI initialization is left to
    <TT>ESMC_Initialize()</TT> it inherits all of the MPI implementation 
    dependent limitations of what may or may not be done before 
    <TT>MPI_Init()</TT>. For instance, it is unsafe for some MPI implementations,
    such as MPICH, to do IO before the MPI environment is initialized. Please
    consult the documentation of your MPI implementation for details.

<P>
Before exiting the application
    the user must call <TT>ESMC_Finalize()</TT> to release resources 
    and clean up the ESMF gracefully.

<P>
The arguments are:
    <DL>
<DT><STRONG>[rc]</STRONG></DT>
<DD>Return code; equals <TT>ESMF_SUCCESS</TT> if there are no errors.
    
</DD>
<DT><STRONG>[defaultConfigFilename]</STRONG></DT>
<DD>Name of the default configuration file for the entire application.
    
</DD>
</DL>

<P>

<P>

<H1><A NAME="SECTION03030000000000000000">
5 GridComp Class</A>
</H1>

<P>

<H2><A NAME="SECTION03031000000000000000">
5.1 Description</A>
</H2>

<A NAME="sec:GridComp"></A>

In Earth system modeling, the most natural way to think about an ESMF 
Gridded Component, or <TT>ESMF_GridComp</TT>, is as a piece of code 
representing a particular physical domain; for example, an atmospheric 
model or an ocean model.  Gridded Components may also represent individual
processes, such as radiation or chemistry.  It's up to the application
writer to decide how deeply to ``componentize.''

Earth system software components tend to share a number of basic 
features.  Most ingest and produce a variety of physical fields; refer to 
a (possibly noncontiguous) spatial region and a grid that is 
partitioned across a set of computational resources; and require 
a clock, usually for stepping a governing set of PDEs forward in time.  
Most can also be divided into distinct initialize, run, and finalize 
computational phases.  These common characteristics are used 
within ESMF to define a Gridded Component data structure that 
is tailored for Earth system modeling and yet is still flexible
enough to represent a variety of domains.

A well-designed Gridded Component does not store information 
internally about how it couples to other Gridded Components.  That
allows it to be used in different contexts without changes to source
code.  The idea here is to avoid situations in which slightly
different versions of the same model source are maintained for use in 
different contexts - standalone vs. coupled versions, for example.
Data is passed between Gridded Components using an intermediary 
Coupler Component, described in Section <A HREF="node3.html#sec:CplComp">6.1</A>.

An ESMF Gridded Component has two parts, one which is user-written
and another which is part of the framework.  The user-written
part is software that represents a physical domain or performs some
other computational function.  It forms the body of the Gridded 
Component.  It may be a piece of legacy code, or it may be developed 
expressly for use with the ESMF.  It must contain routines with
standard ESMF interfaces that can be called to initialize, run, and
finalize the Gridded Component.  These routines can have separate 
callable phases, such as distinct first and second initialization steps.

The part provided by ESMF is the Gridded Component derived type 
itself, <TT>ESMF_GridComp</TT>.  An <TT>ESMF_GridComp</TT> must be created 
for every portion of the application that will be represented 
as a separate component; for example, in a climate model, there may 
be Gridded Components representing the land, ocean, sea ice, and 
atmosphere.  If the application contains an ensemble of identical 
Gridded Components, every one has its own associated <TT>ESMF_GridComp</TT>.
Each Gridded Component has its own name and is allocated
a set of computational resources, in the form of an ESMF Virtual
Machine, or VM.

The user-written part of a Gridded Component is associated with an
<TT>ESMF_GridComp</TT> derived type through a routine called SetServices.
This is a routine that the user must write, and declare public.
Inside the SetServices routine the user must call  
<TT>ESMF_SetEntryPoint</TT> methods that associate a standard ESMF 
operation with the name of the corresponding Fortran subroutine in their user code.


<P>

<P>
A Gridded Component is a computational entity which consumes and produces data. It uses a State object to exchange data between itself and other Components. It uses a Clock object to manage time, and a VM to describe its own and its child components' computational resources.

<P>
This section shows how to create Gridded Components.  For demonstrations
of the use of Gridded Components, see the system tests that are bundled with the ESMF software
distribution.  These can be found in the directory <TT>esmf/src/system_tests</TT>.

<P>

<P>

<H2><A NAME="SECTION03032000000000000000">
5.2 Class API</A>
</H2>

<P>

<P>

<P>

<H2><A NAME="SECTION03033000000000000000">
5.3 C++:  Class Interface ESMC_Comp - Public C interface to the ESMF Comp class (Source File: ESMC_Comp.h)</A>
</H2>

<P>
The code in this file defines the public C Comp class and declares global
   variables to be used in user code written in C.

<P>

<P>

<H1><A NAME="SECTION03040000000000000000">
6 CplComp Class</A>
</H1>

<P>

<H2><A NAME="SECTION03041000000000000000">
6.1 Description</A>
</H2>

<P>
<A NAME="sec:CplComp"></A>
<P>
In a large, multi-component application such as a weather 
forecasting or climate prediction system running within ESMF, 
physical domains and major system functions are represented 
as Gridded Components 
(see Section <A HREF="node3.html#sec:GridComp">5.1</A>).  A Coupler Component, or 
<TT>ESMF_CplComp</TT>, arranges and executes the data 
transformations between the Gridded Components.  Ideally, 
Coupler Components should contain all the information 
about inter-component communication for an application.
This enables the Gridded Components in the application to be 
used in multiple contexts; that is, used in different coupled 
configurations without changes to their source code. 
For example, the same atmosphere might in one case be coupled 
to an ocean in a hurricane prediction model, and in another
coupled to a data assimilation system for numerical weather 
prediction.

<P>
Like Gridded Components, Coupler Components have two parts, one
that is provided by the user and another that is part of the 
framework.  The user-written portion of the software is the coupling
code necessary for a particular exchange between Gridded Components.  
The term ``user-written'' is somewhat misleading here, since within 
a Coupler Component the user can leverage ESMF infrastructure 
software for regridding, redistribution, lower-level communications, 
calendar management, and other functions.  However, ESMF is unlikely 
to offer all the software necessary to customize a data transfer
between Gridded Components.  ESMF does not currently offer tools 
for unit tranformations or time averaging operations, so users 
must manage those operations themseves.

<P>
The user-written Coupler Component code must be divided into 
separately callable initialize, run, and finalize methods.  The 
interfaces for these methods are prescribed by ESMF.

<P>
The second part of a Coupler Component is the <TT>ESMF_CplComp</TT>
derived type within ESMF.  The user must create one of these types
to represent a specific coupling function, such as the regular
transfer of data between a data assimilation system and an 
atmospheric model.  <A NAME="tex2html8"
  HREF="footnode.html#foot848"><SUP>1</SUP></A>
<P>
The user-written part of a Coupler Component is associated with an
<TT>ESMF_CplComp</TT> derived type through a routine called SetServices.
This is a routine that the user must write, and declare public.
Inside the SetServices routine the user must call 
<TT>ESMF_SetEntryPoint</TT> methods that associate a standard ESMF 
operation with the name of the corresponding Fortran subroutine in 
their user code.  For example, a user routine called ``couplerInit''
might be associated with the standard initialize routine in a 
Coupler Component.

<P>
Coupler Components can be written to transform data between a 
pair of Gridded Components, or a single Coupler Component can couple 
more than two Gridded Components.

<P>

<P>
A Coupler Component manages the transformation of data between Components.
It contains a list of State objects and the operations needed to
make them compatible, including such things as regridding and unit conversion.
Coupler Components are user-written, following prescribed ESMF interfaces
and, wherever desired, using ESMF infrastructure tools.

<P>

<P>

<OL>
<LI><B>No optional arguments.</B> User-written routines called by SetServices,
and registered for Initialize, Run and Finalize, <EM>must not</EM> declare any
of the arguments as optional.

<P>
</LI>
<LI><B>No Transforms.</B>  Components must exchange data through 
<TT>ESMF_State</TT> objects.  The input data are available at the time 
the component code is called, and data to be returned to another 
component are available when that code returns.  

<P>
</LI>
<LI><B>No automatic unit conversions.</B>  The ESMF framework does not 
currently contain tools for performing unit conversions, operations that 
are fairly standard within Coupler Components.

<P>
</LI>
<LI><B>No accumulator.</B>  The ESMF does not have an accumulator tool, to
perform time averaging of fields for coupling.  This is likely to be developed
in the near term.

<P>
</LI>
</OL>

<P>

<P>

<H1><A NAME="SECTION03050000000000000000">
7 State Class</A>
</H1>

<P>

<H2><A NAME="SECTION03051000000000000000">
7.1 Description</A>
</H2>

<P>
A State contains the data and metadata to be transferred between 
ESMF components.  It is an important class, because it defines a 
standard for how data is represented in data transfers between Earth
science Components.  The 
State construct is a rational compromise between a fully prescribed 
interface - one that would dictate what specific fields should be 
transferred between components - and an interface in which data structures
are completely ad hoc.

<P>
There are two types of States, import and export.
An import State contains data that is necessary for a Gridded Component
or Coupler Component to execute, and an export State contains the data
that a Gridded Component or Coupler Component can make available.

<P>
States can contain Arrays, ArrayBundles, Fields, FieldBundles, and other States.  They
cannot directly contain Fortran arrays.   Objects in a State must span
the VM on which they are running.  For sequentially executing components
which run on the same set of PETs this happens by calling the object
create methods on each PET, creating the object in unison.   For
concurrently executing components which are running on subsets of PETs,
an additional reconcile method is provided by the ESMF to broadcast information
about objects which were created in sub-components.

<P>
State methods include creation and deletion, adding and retrieving 
data items, adding and retrieving attributes, and performing queries.  

<P>

<P>

<P>

<OL>
<LI><B>Flags not fully implemented.</B>
The flags for indicating various qualities associated with 
data items in a State - validity, whether or not the item is
required for restart, read/write status - are not fully implemented.
Although their defaults can be set, the associated methods for 
setting and getting these flags have not been implemented.
(The <TT>needed</TT> flag is fully supported.)

<P>
</LI>
<LI><B>No synchronization at object create time.</B>
Object IDs are using during the reconcile process to identify objects
which are unknown to some subset of the PETs in the currently running VM.
Object IDs are assigned in sequential order at object create time.
User input at design time requested there be no communication overhead
during the create of an object, so there is no opportunity to
synchronize IDs if one or more PETs create objects which
are not in unison (not all PETs in the VM make the same calls).

<P>
Even if the user follows the unison rules, if components are running on 
a subset of the PETs, when they return to the parent (calling) component
the next available ID will potentially not be the same across all
PETs in the VM.  Part of the reconcile process or part of the return
to the parent will need to have a broadcast which sends the current
ID number, and all PETs can reset the next available number to the highest
number broadcast.  This could be an async call to avoid as much as
possible serialization and barrier issues.

<P>
Default object names are based on the object id (e.g. "Field1", "Field2")
to create unique object names, so basing the detection of unique objects 
on the name instead of on the object id is no better solution.

<P>
</LI>
</OL>

<P>

<P>

<H2><A NAME="SECTION03052000000000000000">
7.2 Class API</A>
</H2>

<P>

<P>

<P>

<H2><A NAME="SECTION03053000000000000000">
7.3 C++:  Class Interface ESMC_State - C interface to the F90 State object (Source File: ESMC_State.h)</A>
</H2>

<P>
The code in this file defines the public C State

<P>

<P>

<HR>
<!--Navigation Panel-->
<A NAME="tex2html267"
  HREF="node4.html">
<IMG WIDTH="37" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="next" SRC="next.png"></A> 
<A NAME="tex2html263"
  HREF="ESMC_crefdoc.html">
<IMG WIDTH="26" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="up" SRC="up.png"></A> 
<A NAME="tex2html257"
  HREF="node2.html">
<IMG WIDTH="63" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="previous" SRC="prev.png"></A> 
<A NAME="tex2html265"
  HREF="node1.html">
<IMG WIDTH="65" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="contents" SRC="contents.png"></A>  
<BR>
<B> Next:</B> <A NAME="tex2html268"
  HREF="node4.html">3 Infrastructure: Fields and</A>
<B> Up:</B> <A NAME="tex2html264"
  HREF="ESMC_crefdoc.html">ESMC_crefdoc</A>
<B> Previous:</B> <A NAME="tex2html258"
  HREF="node2.html">1 ESMF Overview</A>
 &nbsp <B>  <A NAME="tex2html266"
  HREF="node1.html">Contents</A></B> 
<!--End of Navigation Panel-->
<ADDRESS>
<A HREF=mailto:esmf_support@list.woc.noaa.gov>esmf_support@list.woc.noaa.gov</A>
</ADDRESS>
</BODY>
</HTML>
